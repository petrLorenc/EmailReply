{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#python -m nltk.downloader all\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "\n",
    "import pickle\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "size_glove = 100\n",
    "\n",
    "# using coordinate-wise minimum - minimum from all first elements in vector and so on\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def load_stop_words(path_to_file):\n",
    "    with open(path_to_file, mode=\"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        lines = file.readlines()\n",
    "    return [stopword.strip() for stopword in lines]\n",
    "    \n",
    "def get_lemmatized_words(sentence, stopwords):\n",
    "    _temp = []\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    for word, token in nltk.pos_tag(tokens):\n",
    "        if word.strip() not in stopwords:\n",
    "            _temp.append(lmtzr.lemmatize(word.strip().lower(), get_wordnet_pos(token)))\n",
    "    return \" \".join(_temp)\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = np.array(embedding).reshape(1,-1)\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def sentence_2_vec(model, sentence):\n",
    "    vectors = []\n",
    "    for word in sentence.split(): \n",
    "        try:\n",
    "            vectors.append(model[word.strip().lower()][0])\n",
    "        except:\n",
    "            pass\n",
    "    #print(vectors)\n",
    "    vectors = np.matrix(np.array(vectors).reshape(-1, size_glove))\n",
    "    #print(vectors)\n",
    "    maxVector = []\n",
    "    minVector = []\n",
    "    for i in range(size_glove):\n",
    "        maxVector.append(np.max(vectors[:,i]))\n",
    "        minVector.append(np.min(vectors[:,i]))\n",
    "    return maxVector + minVector\n",
    "\n",
    "def process_file(X_train, model):\n",
    "    result_of_processing = []\n",
    "    \n",
    "    for sentence_in_X in X_train:\n",
    "        vectors = sentence_2_vec(model, sentence=sentence_in_X)\n",
    "        result_of_processing.append(vectors)\n",
    "        \n",
    "    return result_of_processing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "model = loadGloveModel(\"./glove_data/glove.6B.\" + str(size_glove) + \"d.txt\")\n",
    "X_train, y_train = load_file_to_process(\"./data/train_5500.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "post_X_train = np.array(process_file(X_train, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 200)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_X_traint_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5452"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do serfdom develop in and then leave Russia ?',\n",
       " 'What film feature the character Popeye Doyle ?',\n",
       " \"How can I find a list of celebrity ' real name ?\",\n",
       " 'What fowl grab the spotlight after the Chinese Year of the Monkey ?',\n",
       " 'What be the full form of .com ?',\n",
       " 'What contemptible scoundrel steal the cork from my lunch ?',\n",
       " \"What team do baseball 's St. Louis Browns become ?\",\n",
       " 'What be the old profession ?',\n",
       " 'What be liver enzyme ?',\n",
       " 'Name the scar-faced bounty hunter of The Old West .']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "size_glove = 100\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = np.array(embedding).reshape(1,-1)\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "model = loadGloveModel(\"./glove_data/glove.6B.\" + str(size_glove) + \"d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sentence_2_vec(model, sentence):\n",
    "    vectors = []\n",
    "    for word in sentence.split(): \n",
    "        try:\n",
    "            vectors.append(model[word.strip().lower()][0])\n",
    "        except:\n",
    "            pass\n",
    "    #print(vectors)\n",
    "    vectors = np.matrix(np.array(vectors).reshape(-1, size_glove))\n",
    "    #print(vectors)\n",
    "    maxVector = []\n",
    "    minVector = []\n",
    "    for i in range(size_glove):\n",
    "        maxVector.append(np.max(vectors[:,i]))\n",
    "        minVector.append(np.min(vectors[:,i]))\n",
    "    return maxVector + minVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-4786c4310cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_2_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-7f60908995a0>\u001b[0m in \u001b[0;36msentence_2_vec\u001b[0;34m(model, sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msentence_2_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "sentence_2_vec(model, x_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_file_to_process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-6976f2cb75be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file_to_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/train_5500.label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_file_to_process' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_file_to_process(\"./data/train_5500.label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__loss': ('perceptron', 'hinge'),\n",
      " 'clf__n_iter': (10, 25),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__use_idf': (True, False),\n",
      " 'vect__max_features': (None, 5000, 10000, 50000),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (2, 2)),\n",
      " 'vect__stop_words': ({'english'}, None)}\n",
      "Fitting 3 folds for each of 768 candidates, totalling 2304 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2304 out of 2304 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 371.436s\n",
      "\n",
      "Best score: 0.866\n",
      "Best parameters set:\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__n_iter: 25\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_features: 10000\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__stop_words: {'english'}\n"
     ]
    }
   ],
   "source": [
    "pipeline1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()), # Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__stop_words': ({'english'}, None),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (2, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__loss': ('perceptron', 'hinge'),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__n_iter': (10, 25),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline1, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline1.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['strip_accents', 'vocabulary', 'input', 'lowercase', 'analyzer', 'max_df', 'token_pattern', 'norm', 'decode_error', 'min_df', 'binary', 'max_features', 'preprocessor', 'stop_words', 'use_idf', 'tokenizer', 'sublinear_tf', 'smooth_idf', 'dtype', 'encoding', 'ngram_range'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVectorizer().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__loss': ('perceptron', 'hinge'),\n",
      " 'clf__n_iter': (10, 25),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'tfidf__max_df': (0.5, 1.0, 2.0),\n",
      " 'tfidf__max_features': (None, 5000, 10000, 50000),\n",
      " 'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)),\n",
      " 'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__stop_words': ({'english'}, None),\n",
      " 'tfidf__use_idf': (True, False)}\n",
      "Fitting 3 folds for each of 2304 candidates, totalling 6912 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4992 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6042 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6912 out of 6912 | elapsed: 18.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1125.914s\n",
      "\n",
      "Best score: 0.868\n",
      "Best parameters set:\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__n_iter: 25\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\ttfidf__max_df: 1.0\n",
      "\ttfidf__max_features: 10000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__stop_words: {'english'}\n",
      "\ttfidf__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "pipeline1 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SGDClassifier()), # Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'tfidf__stop_words': ({'english'}, None),\n",
    "    'tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__max_df': (0.5, 1.0 , 2.0), # ignore terms that have a document frequency strictly higher than the given threshold\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__loss': ('perceptron', 'hinge'),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__n_iter': (10, 25),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline1, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline1.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strip_accents', 'vocabulary', 'input', 'lowercase', 'analyzer', 'max_df', 'token_pattern', 'decode_error', 'min_df', 'binary', 'max_features', 'preprocessor', 'stop_words', 'tokenizer', 'dtype', 'encoding', 'ngram_range']\n"
     ]
    }
   ],
   "source": [
    "print ([key for key in TfidfVectorizer().get_params().keys() if key in CountVectorizer().get_params().keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='mahalanobis',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighborsClassifier(metric=\"mahalanobis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__lowercase': (True, False),\n",
      " 'tfidf__max_df': (0.5, 1.0, 2.0),\n",
      " 'tfidf__max_features': (None, 5000, 10000, 50000),\n",
      " 'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)),\n",
      " 'tfidf__stop_words': ({'english'}, None)}\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 80.396s\n",
      "\n",
      "Best score: 0.853\n",
      "Best parameters set:\n",
      "\ttfidf__lowercase: False\n",
      "\ttfidf__max_df: 0.5\n",
      "\ttfidf__max_features: None\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__lowercase': (True, False),\n",
      " 'tfidf__max_df': (0.5, 1.0, 2.0),\n",
      " 'tfidf__max_features': (None, 5000, 10000, 50000),\n",
      " 'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)),\n",
      " 'tfidf__stop_words': ({'english'}, None)}\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 64.436s\n",
      "\n",
      "Best score: 0.867\n",
      "Best parameters set:\n",
      "\ttfidf__lowercase: False\n",
      "\ttfidf__max_df: 0.5\n",
      "\ttfidf__max_features: None\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__lowercase': (True, False),\n",
      " 'tfidf__max_df': (0.5, 1.0, 2.0),\n",
      " 'tfidf__max_features': (None, 5000, 10000, 50000),\n",
      " 'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)),\n",
      " 'tfidf__stop_words': ({'english'}, None)}\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 199.349s\n",
      "\n",
      "Best score: 0.635\n",
      "Best parameters set:\n",
      "\ttfidf__lowercase: True\n",
      "\ttfidf__max_df: 1.0\n",
      "\ttfidf__max_features: 5000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__lowercase': (True, False),\n",
      " 'tfidf__max_df': (0.5, 1.0, 2.0),\n",
      " 'tfidf__max_features': (None, 5000, 10000, 50000),\n",
      " 'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)),\n",
      " 'tfidf__stop_words': ({'english'}, None)}\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 191.362s\n",
      "\n",
      "Best score: 0.757\n",
      "Best parameters set:\n",
      "\ttfidf__lowercase: False\n",
      "\ttfidf__max_df: 1.0\n",
      "\ttfidf__max_features: 5000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: {'english'}\n",
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__lowercase': (True, False),\n",
      " 'tfidf__max_df': (0.5, 1.0, 2.0),\n",
      " 'tfidf__max_features': (None, 5000, 10000, 50000),\n",
      " 'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)),\n",
      " 'tfidf__stop_words': ({'english'}, None)}\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 139.894s\n",
      "\n",
      "Best score: 0.601\n",
      "Best parameters set:\n",
      "\ttfidf__lowercase: True\n",
      "\ttfidf__max_df: 0.5\n",
      "\ttfidf__max_features: 5000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__lowercase': (True, False),\n",
      " 'tfidf__max_df': (0.5, 1.0, 2.0),\n",
      " 'tfidf__max_features': (None, 5000, 10000, 50000),\n",
      " 'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)),\n",
      " 'tfidf__stop_words': ({'english'}, None)}\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 151.636s\n",
      "\n",
      "Best score: 0.573\n",
      "Best parameters set:\n",
      "\ttfidf__lowercase: True\n",
      "\ttfidf__max_df: 0.5\n",
      "\ttfidf__max_features: 5000\n",
      "\ttfidf__ngram_range: (1, 1)\n",
      "\ttfidf__stop_words: None\n",
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__lowercase': (True, False),\n",
      " 'tfidf__max_df': (0.5, 1.0, 2.0),\n",
      " 'tfidf__max_features': (None, 5000, 10000, 50000),\n",
      " 'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)),\n",
      " 'tfidf__stop_words': ({'english'}, None)}\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\n",
    "    # Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification)\n",
    "    \"MultinomialNB\" :MultinomialNB(),\n",
    "    \"KNeighborsClassifier\":KNeighborsClassifier(),\n",
    "    \n",
    "    # Classifier implementing a vote among neighbors within a given radius\n",
    "    \"RadiusNeighborsClassifier\":RadiusNeighborsClassifier(),\n",
    "    \n",
    "    # Multi-layer Perceptron classifier.\n",
    "    \"MLPClassifier\":MLPClassifier(activation='relu', solver='sgd', learning_rate='adaptive'),\n",
    "    \n",
    "    # A random forest is a meta estimator that fits a number of decision tree classifiers \n",
    "    # on various sub-samples of the dataset and use averaging to improve the predictive \n",
    "    # accuracy and control over-fitting.\n",
    "    \"RandomForestClassifier\":RandomForestClassifier(min_samples_leaf=10),\n",
    "    \n",
    "    #An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier \n",
    "    # on the original dataset and then fits additional copies of the classifier on the same dataset \n",
    "    # but where the weights of incorrectly classified instances are adjusted such that subsequent \n",
    "    # classifiers focus more on difficult cases.\n",
    "    \"AdaBoostClassifier\":AdaBoostClassifier(),\n",
    "    \"SGDClassifier\":SGDClassifier()\n",
    "} \n",
    "\n",
    "vectorizers = {\n",
    "    # Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification)\n",
    "    \"TfidfVectorizer\" :TfidfVectorizer(),\n",
    "    \"CountVectorizer\":CountVectorizer()\n",
    "}\n",
    "\n",
    "best_scores = []\n",
    "\n",
    "for name_classifier, classifier in classifiers.items():\n",
    "    for name_vectorizer, vectorizer in vectorizers.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', vectorizer),\n",
    "            ('clf', classifier) ])\n",
    "\n",
    "        parameters = {\n",
    "            'tfidf__stop_words': ({'english'}, None),\n",
    "            'tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "            'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)),  # unigrams or bigrams\n",
    "            'tfidf__lowercase': (True, False),\n",
    "            'tfidf__max_df': (0.5, 1.0 , 2.0), # ignore terms that have a document frequency strictly higher than the given threshold\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "        print(\"Performing grid search...\")\n",
    "        print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "        print(\"parameters:\")\n",
    "        pprint(parameters)\n",
    "        t0 = time()\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print()\n",
    "\n",
    "        best_scores.append((name_classifier, name_vectorizer, grid_search.best_score_, (time() - t0), grid_search.best_estimator_.get_params()))\n",
    "\n",
    "        print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "        print(\"Best parameters set:\")\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "            \n",
    "print (best_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
