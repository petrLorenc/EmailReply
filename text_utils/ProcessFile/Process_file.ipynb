{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "with open(\"./train_5500.label\", mode=\"r\", encoding=\"ISO-8859-1\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "#nltk.download('all')\n",
    "\n",
    "pattern = r\"^([A-Z]*):[a-z]* (.*)\"\n",
    "\n",
    "input_dimenstion = 100\n",
    "\n",
    "input_x = []\n",
    "input_y = []\n",
    "for line in lines:\n",
    "    matchObj = re.match(pattern, line.strip())\n",
    "    input_x.append(matchObj.group(2))\n",
    "    input_y.append(matchObj.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('how', 'WRB'), ('be', 'VB'), ('you', 'PRP'), (',', ','), ('you', 'PRP'), ('big', 'JJ'), ('boys', 'NNS')]\n",
      "[('how', 'ADV'), ('be', 'VERB'), ('you', 'PRON'), (',', '.'), ('you', 'PRON'), ('big', 'ADJ'), ('boys', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "text_pos = nltk.pos_tag(nltk.word_tokenize(\"how be you, you big boys\"))\n",
    "print (text_pos)\n",
    "simplifiedTags = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in text_pos]\n",
    "print(simplifiedTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  What/WP\n",
      "  (NP fowl/NN)\n",
      "  grabs/VBD\n",
      "  (NP the/DT spotlight/NN)\n",
      "  after/IN\n",
      "  (NP the/DT Chinese/JJ Year/NN)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  Monkey/NNP\n",
      "  ?/.)\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN|NNS>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(text_pos)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP What/PRON fowl/NOUN)\n",
      "  grabs/VERB\n",
      "  the/DET\n",
      "  (NP spotlight/NOUN)\n",
      "  after/ADP\n",
      "  the/DET\n",
      "  Chinese/ADJ\n",
      "  (NP Year/NOUN)\n",
      "  of/ADP\n",
      "  the/DET\n",
      "  (NP Monkey/NOUN)\n",
      "  ?/.)\n",
      "(S\n",
      "  What/PRON\n",
      "  (NP fowl/NOUN)\n",
      "  grabs/VERB\n",
      "  (NP the/DET spotlight/NOUN)\n",
      "  after/ADP\n",
      "  (NP the/DET Chinese/ADJ Year/NOUN)\n",
      "  of/ADP\n",
      "  (NP the/DET Monkey/NOUN)\n",
      "  ?/.)\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<PRON>?<NOUN>}\"\n",
    "grammar2 = \"NP: {<DET>?<ADJ>?<NOUN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cp2 = nltk.RegexpParser(grammar2)\n",
    "result = cp.parse(simplifiedTags)\n",
    "print(result)\n",
    "result2 = cp2.parse(simplifiedTags)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Monkey': 'NE', 'Chinese': 'NE'}\n",
      "{'Monkey': 'GPE', 'Chinese': 'GPE'}\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(input_x[3])\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "def extractEntities(ne_chunked):\n",
    "    data = {}\n",
    "    for entity in ne_chunked:\n",
    "        if isinstance(entity, nltk.tree.Tree):\n",
    "            text = \" \".join([word for word, tag in entity.leaves()])\n",
    "            ent = entity.label()\n",
    "            data[text] = ent\n",
    "        else:\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "ne_chunked = nltk.ne_chunk(tagged, binary=True)\n",
    "print(extractEntities(ne_chunked))\n",
    "ne_chunked = nltk.ne_chunk(tagged, binary=False)\n",
    "print(extractEntities(ne_chunked))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def load_stop_words(path_to_file):\n",
    "    with open(path_to_file, mode=\"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        lines = file.readlines()\n",
    "    return [stopword.strip() for stopword in lines]\n",
    "    \n",
    "def get_lemmatized_words(sentence, stopwords):\n",
    "    _temp = []\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    for word, token in nltk.pos_tag(tokens):\n",
    "        if word.strip() not in stopwords:\n",
    "            _temp.append(lmtzr.lemmatize(word.strip().lower(), get_wordnet_pos(token)))\n",
    "    return \" \".join(_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"./train_5500.label\", mode=\"r\", encoding=\"ISO-8859-1\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "#nltk.download()\n",
    "\n",
    "pattern = r\"^([A-Z]*):[a-z]* (.*)\"\n",
    "\n",
    "text = []\n",
    "\n",
    "input_x = []\n",
    "input_y = []\n",
    "for line in lines:\n",
    "    matchObj = re.match(pattern, line.strip())\n",
    "    text.append((matchObj.group(2), matchObj.group(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for _, label in text if label == \"ABBR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1162"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for _, label in text if label == \"DESC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for _, label in text if label == \"ENTY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1223"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for _, label in text if label == \"HUM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for _, label in text if label == \"LOC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for _, label in text if label == \"NUM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./train_5500.label\", mode=\"r\", encoding=\"ISO-8859-1\") as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "max([len(line.split(\" \")) for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
