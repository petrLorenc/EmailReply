{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n",
      "Processing text dataset\n",
      "Found 5452 texts.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Embedding, LSTM, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "#pip install h5py\n",
    "EMBEDDING_DIM = 300\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 10\n",
    "LONGEST_SENTENCE = 39\n",
    "\n",
    "#GLOVE_DIR = \"./glove_data/glove.6B.\" + str(EMBEDDING_DIM) + \"d.txt\"\n",
    "GLOVE_DIR = \"../QuestionClassificationScikit/glove_data/glove.6B.\" + str(EMBEDDING_DIM) + \"d.txt\"\n",
    "\n",
    "TEXT_DATA = \"./train_5500.label.text\"\n",
    "TAG_DATA = \"./train_5500.label.tag\"\n",
    "SAVE_DIR = \"./train_model\"\n",
    "NAME_MODEL = \"model_LSTM\"\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = LONGEST_SENTENCE # longest sentences\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "texts = []  # list of text samples\n",
    "mapping2 = {'ABBR' : 0, \n",
    "               'DESC' : 1,\n",
    "               'ENTY' : 2,\n",
    "               'HUM' : 3,\n",
    "               'LOC' : 4,\n",
    "               'NUM' : 5} # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "        print (\"Loading Glove Model\")\n",
    "        f = open(gloveFile,'r', encoding=\"UTF-8\")\n",
    "        model = {}\n",
    "        for line in f:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = [float(val) for val in splitLine[1:]]\n",
    "            model[word] = np.array(embedding).reshape(1,-1)\n",
    "        print (\"Done.\",len(model),\" words loaded!\")\n",
    "        return model\n",
    "\n",
    "embeddings_index = loadGloveModel(GLOVE_DIR)\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "with open(TEXT_DATA, mode=\"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        texts = file.readlines()\n",
    "\n",
    "with open(TAG_DATA, mode=\"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        labels = [mapping2[tag.strip()] for tag in file.readlines()]\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open(\"./train_model/model_CNN_300_10_reg_bigger_window.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"./train_model/model_CNN_300_10_reg_bigger_window.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_mapping = {0: [\"I am not sure.\",\"What?\"], \n",
    "               1: [\"Sounds right.\",\"Please, tell me more.\"],\n",
    "               2: [\"I will take it.\" ,\"This type of things is strange.\"],\n",
    "               3: [\"I dont know him.\", \"Ok\", \"No\", \"He is right next to me.\"],\n",
    "               4: [\"I will be there\", \"I know that place\", \"It is near to me.\"],\n",
    "               5: [\"That is correct.\", \"Very big numbers.\", \"\"]} # dictionary mapping label name to numeric id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_mapping2 = {v: k for k, v in mapping2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUM\n",
      "Answers : ['I dont know him.', 'Ok', 'No', 'He is right next to me.']\n"
     ]
    }
   ],
   "source": [
    "test_text = [\"who are you\"]\n",
    "sequence = tokenizer.texts_to_sequences(test_text)\n",
    "data = pad_sequences(np.array(sequence), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "choose = np.argmax(loaded_model.predict(np.array(data)))\n",
    "print(inv_mapping2[choose])\n",
    "print(\"Answers :\" , inv_mapping[choose])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC\n",
      "Answers : ['I will be there', 'I know that place', 'It is near to me.']\n"
     ]
    }
   ],
   "source": [
    "test_text = [\"Where are you from?\"]\n",
    "sequence = tokenizer.texts_to_sequences(test_text)\n",
    "data = pad_sequences(np.array(sequence), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "choose = np.argmax(loaded_model.predict(np.array(data)))\n",
    "print(inv_mapping2[choose])\n",
    "print(\"Answers :\" , inv_mapping[choose])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESC\n",
      "Answers : ['Sounds right.', 'Please, tell me more.']\n"
     ]
    }
   ],
   "source": [
    "test_text = [\"Is there something like god?\"]\n",
    "sequence = tokenizer.texts_to_sequences(test_text)\n",
    "data = pad_sequences(np.array(sequence), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "choose = np.argmax(loaded_model.predict(np.array(data)))\n",
    "print(inv_mapping2[choose])\n",
    "print(\"Answers :\" , inv_mapping[choose])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
